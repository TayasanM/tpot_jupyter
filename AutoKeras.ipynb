{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8316906c-25d3-4984-968c-b71776491037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF GPUs: []\n",
      "Train labels: (array([0, 1]), array([4993, 4933]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 13:04:31.919945: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-18 13:04:31.921520: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expect the data to TextInput to be strings, but got object.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# --- AutoKeras Text AutoML\u001b[39;00m\n\u001b[1;32m     39\u001b[0m clf \u001b[38;5;241m=\u001b[39m ak\u001b[38;5;241m.\u001b[39mTextClassifier(\n\u001b[1;32m     40\u001b[0m     max_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,        \u001b[38;5;66;03m# increase (e.g., 30-50) for better results\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     42\u001b[0m )\n\u001b[0;32m---> 44\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# increase (e.g., 10-20) for better results\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# --- Evaluate\u001b[39;00m\n\u001b[1;32m     51\u001b[0m proba \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(x_test)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/tpot_jupyter/miniconda3/envs/ak_env/lib/python3.10/site-packages/autokeras/tasks/text.py:163\u001b[0m, in \u001b[0;36mTextClassifier.fit\u001b[0;34m(self, x, y, epochs, callbacks, validation_split, validation_data, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    108\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    115\u001b[0m ):\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search for the best model and hyperparameters for the AutoModel.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    It will search for the best model based on the performances on\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m            applicable).\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/tpot_jupyter/miniconda3/envs/ak_env/lib/python3.10/site-packages/autokeras/auto_model.py:293\u001b[0m, in \u001b[0;36mAutoModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, callbacks, validation_split, validation_data, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m     validation_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    290\u001b[0m dataset, validation_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_adapt(\n\u001b[1;32m    291\u001b[0m     x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my, validation_data\u001b[38;5;241m=\u001b[39mvalidation_data\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_analyze_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_hyper_pipeline()\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Split the data with validation_split.\u001b[39;00m\n",
      "File \u001b[0;32m~/tpot_jupyter/miniconda3/envs/ak_env/lib/python3.10/site-packages/autokeras/auto_model.py:381\u001b[0m, in \u001b[0;36mAutoModel._analyze_data\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    378\u001b[0m     analyser\u001b[38;5;241m.\u001b[39mupdate(array)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m analyser \u001b[38;5;129;01min\u001b[39;00m analysers:\n\u001b[0;32m--> 381\u001b[0m     \u001b[43manalyser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hm, analyser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_heads, analysers):\n\u001b[1;32m    384\u001b[0m     hm\u001b[38;5;241m.\u001b[39mconfig_from_analyser(analyser)\n",
      "File \u001b[0;32m~/tpot_jupyter/miniconda3/envs/ak_env/lib/python3.10/site-packages/autokeras/analysers/input_analysers.py:56\u001b[0m, in \u001b[0;36mTextAnalyser.finalize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpect the data to TextInput to have shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(batch_size, 1), but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot input shape \u001b[39m\u001b[38;5;132;01m{shape}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     54\u001b[0m     )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpect the data to TextInput to be strings, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{type}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     59\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Expect the data to TextInput to be strings, but got object."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import autokeras as ak\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "# --- read TSV\n",
    "train_df = pd.read_csv(\"new_data_train_Yelp_Fake_Review.csv\", sep=\"\\t\", engine=\"python\")\n",
    "test_df  = pd.read_csv(\"new_data_test_Yelp_Fake_Review.csv\",  sep=\"\\t\", engine=\"python\")\n",
    "\n",
    "train_df = train_df[[\"reviewContent\", \"flagged\"]].dropna().reset_index(drop=True)\n",
    "test_df  = test_df[[\"reviewContent\", \"flagged\"]].dropna().reset_index(drop=True)\n",
    "\n",
    "# --- (light) text cleaning\n",
    "_url_re = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "_space_re = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = s.replace(\"\\t\", \" \").replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    s = _url_re.sub(\" URL \", s)\n",
    "    s = _space_re.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "x_train = train_df[\"reviewContent\"].astype(str).map(clean_text).to_numpy()\n",
    "y_train = train_df[\"flagged\"].astype(int).to_numpy()\n",
    "\n",
    "x_test  = test_df[\"reviewContent\"].astype(str).map(clean_text).to_numpy()\n",
    "y_test  = test_df[\"flagged\"].astype(int).to_numpy()\n",
    "\n",
    "print(\"TF GPUs:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "print(\"Train labels:\", np.unique(y_train, return_counts=True))\n",
    "\n",
    "\n",
    "# --- AutoKeras Text AutoML\n",
    "clf = ak.TextClassifier(\n",
    "    max_trials=10,        # increase (e.g., 30-50) for better results\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "clf.fit(\n",
    "    x_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=5              # increase (e.g., 10-20) for better results\n",
    ")\n",
    "\n",
    "# --- Evaluate\n",
    "proba = clf.predict(x_test).reshape(-1)\n",
    "y_pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1:\", f1_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# --- Save\n",
    "clf.export_model().save(\"autokeras_yelp_text_model\", save_format=\"tf\")\n",
    "print(\"Saved: autokeras_yelp_text_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259acf1d-da81-4396-bf8b-c63ce1c71a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 28 Complete [00h 00m 10s]\n",
      "val_loss: 0.6728435754776001\n",
      "\n",
      "Best val_loss So Far: 0.67254239320755\n",
      "Total elapsed time: 00h 07m 29s\n",
      "\n",
      "Search: Running Trial #29\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "500               |500               |text_block_1/max_tokens\n",
      "32                |32                |text_block_1/embedding_1/embedding_dim\n",
      "0                 |0                 |text_block_1/embedding_1/dropout\n",
      "3                 |3                 |text_block_1/conv_block_1/kernel_size\n",
      "False             |False             |text_block_1/conv_block_1/separable\n",
      "False             |False             |text_block_1/conv_block_1/max_pooling\n",
      "1                 |1                 |text_block_1/conv_block_1/num_blocks\n",
      "2                 |2                 |text_block_1/conv_block_1/num_layers\n",
      "512               |64                |text_block_1/conv_block_1/filters_0_0\n",
      "512               |512               |text_block_1/conv_block_1/filters_0_1\n",
      "0.25              |0.25              |text_block_1/conv_block_1/dropout\n",
      "32                |32                |text_block_1/conv_block_1/filters_1_0\n",
      "32                |32                |text_block_1/conv_block_1/filters_1_1\n",
      "flatten           |flatten           |text_block_1/spatial_reduction_1/reduction_type\n",
      "False             |False             |text_block_1/dense_block_1/use_batchnorm\n",
      "3                 |3                 |text_block_1/dense_block_1/num_layers\n",
      "128               |128               |text_block_1/dense_block_1/units_0\n",
      "0                 |0                 |text_block_1/dense_block_1/dropout\n",
      "32                |32                |text_block_1/dense_block_1/units_1\n",
      "0.25              |0.25              |classification_head_1/dropout\n",
      "adam              |adam              |optimizer\n",
      "2e-05             |2e-05             |learning_rate\n",
      "512               |512               |text_block_1/conv_block_1/filters_2_0\n",
      "128               |128               |text_block_1/conv_block_1/filters_2_1\n",
      "64                |64                |text_block_1/dense_block_1/units_2\n",
      "\n",
      "Epoch 1/5\n",
      "\u001b[1m 90/218\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.5178 - loss: 0.6918"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 0) Imports\n",
    "# =========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import autokeras as ak\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 1) Read TRAIN / TEST (TSV)\n",
    "# =========================================\n",
    "train_df = pd.read_csv(\"new_data_train_Yelp_Fake_Review.csv\", sep=\"\\t\", engine=\"python\")\n",
    "test_df  = pd.read_csv(\"new_data_test_Yelp_Fake_Review.csv\",  sep=\"\\t\", engine=\"python\")\n",
    "\n",
    "train_df = train_df[[\"reviewContent\", \"flagged\"]].dropna().reset_index(drop=True)\n",
    "test_df  = test_df[[\"reviewContent\", \"flagged\"]].dropna().reset_index(drop=True)\n",
    "\n",
    "train_df[\"reviewContent\"] = train_df[\"reviewContent\"].astype(str)\n",
    "test_df[\"reviewContent\"]  = test_df[\"reviewContent\"].astype(str)\n",
    "\n",
    "train_df[\"flagged\"] = train_df[\"flagged\"].astype(int)\n",
    "test_df[\"flagged\"]  = test_df[\"flagged\"].astype(int)\n",
    "\n",
    "print(\"Train labels:\", np.unique(train_df[\"flagged\"], return_counts=True))\n",
    "print(\"Test labels :\", np.unique(test_df[\"flagged\"],  return_counts=True))\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 2) Text cleaning\n",
    "# =========================================\n",
    "_url_re = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "_html_re = re.compile(r\"<.*?>\")\n",
    "_space_re = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = s.replace(\"\\t\", \" \").replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    s = _html_re.sub(\" \", s)\n",
    "    s = _url_re.sub(\" URL \", s)\n",
    "    s = _space_re.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# IMPORTANT: AutoKeras TextClassifier wants 1D array/list of strings\n",
    "x_train = train_df[\"reviewContent\"].map(clean_text).astype(str).to_numpy(dtype=str)   # shape (N,)\n",
    "y_train = train_df[\"flagged\"].to_numpy()\n",
    "\n",
    "x_test  = test_df[\"reviewContent\"].map(clean_text).astype(str).to_numpy(dtype=str)    # shape (M,)\n",
    "y_test  = test_df[\"flagged\"].to_numpy()\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape, \"dtype:\", x_train.dtype)\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 3) AutoKeras Text AutoML\n",
    "# =========================================\n",
    "clf = ak.TextClassifier(\n",
    "    max_trials=30,     # increase for better results (30-50)\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "clf.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_split=0.3,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 4) Evaluation\n",
    "# =========================================\n",
    "# For binary classification, predict() often returns probabilities (N,1)\n",
    "proba = clf.predict(x_test)\n",
    "proba = np.array(proba).reshape(-1)\n",
    "\n",
    "y_pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1:\", f1_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 5) Save trained model\n",
    "# =========================================\n",
    "model = clf.export_model()\n",
    "model.save(\"autokeras_yelp_text_model.keras\")\n",
    "print(\" Saved: autokeras_yelp_text_model.keras\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fb79746-9ef0-4749-9600-c68696e39e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: autokeras_yelp_text_model.keras\n"
     ]
    }
   ],
   "source": [
    "model.save(\"autokeras_yelp_text_model.keras\")\n",
    "print(\" Saved: autokeras_yelp_text_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19351d5d-e1fa-45ff-98b1-ebf0c3938a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels: (array([0, 1]), array([4993, 4933]))\n",
      "Test labels : (array([0, 1]), array([1212, 1271]))\n",
      "GPU devices: []\n",
      "Text input shape: (9926, 1)\n",
      "Struct input shape: (9926, 8)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 149\u001b[0m\n\u001b[1;32m    138\u001b[0m automodel \u001b[38;5;241m=\u001b[39m ak\u001b[38;5;241m.\u001b[39mAutoModel(\n\u001b[1;32m    139\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m[text_in, struct_in],\n\u001b[1;32m    140\u001b[0m     outputs\u001b[38;5;241m=\u001b[39mout,\n\u001b[1;32m    141\u001b[0m     max_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,        \u001b[38;5;66;03m# increase to 30–50 later\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    143\u001b[0m )\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# =========================================\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# 6) Train (LIST INPUT — IMPORTANT)\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# =========================================\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m \u001b[43mautomodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mx_train_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_struct\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# increase to 10–20 later\u001b[39;49;00m\n\u001b[1;32m    154\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# =========================================\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# 7) Predict + Evaluate\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# =========================================\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pred \u001b[38;5;241m=\u001b[39m automodel\u001b[38;5;241m.\u001b[39mpredict([x_test_text, x_test_struct])\n",
      "File \u001b[0;32m~/tpot_jupyter/miniconda3/envs/ak_env/lib/python3.10/site-packages/autokeras/auto_model.py:303\u001b[0m, in \u001b[0;36mAutoModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, callbacks, validation_split, validation_data, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m     dataset, validation_data \u001b[38;5;241m=\u001b[39m data_utils\u001b[38;5;241m.\u001b[39msplit_dataset(\n\u001b[1;32m    299\u001b[0m         dataset, validation_split\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    302\u001b[0m x, y \u001b[38;5;241m=\u001b[39m dataset\n\u001b[0;32m--> 303\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/tpot_jupyter/miniconda3/envs/ak_env/lib/python3.10/site-packages/autokeras/engine/tuner.py:203\u001b[0m, in \u001b[0;36mAutoTuner.search\u001b[0;34m(self, epochs, callbacks, validation_split, verbose, **fit_kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Populate initial search space.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m hp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_space()\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_model_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mupdate_space(hp)\n",
      "File \u001b[0;32m~/tpot_jupyter/miniconda3/envs/ak_env/lib/python3.10/site-packages/autokeras/engine/tuner.py:82\u001b[0m, in \u001b[0;36mAutoTuner._prepare_model_build\u001b[0;34m(self, hp, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m x \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     81\u001b[0m y \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 82\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyper_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit((x, y))\n\u001b[1;32m     84\u001b[0m (x, y) \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mtransform((x, y))\n",
      "File \u001b[0;32m~/tpot_jupyter/miniconda3/envs/ak_env/lib/python3.10/site-packages/keras_tuner/src/engine/hypermodel.py:120\u001b[0m, in \u001b[0;36mHyperModel._build_wrapper\u001b[0;34m(self, hp, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtunable:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Copy `HyperParameters` object so that new entries are not added\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# to the search space.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     hp \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tpot_jupyter/miniconda3/envs/ak_env/lib/python3.10/site-packages/autokeras/pipeline.py:64\u001b[0m, in \u001b[0;36mHyperPipeline.build\u001b[0;34m(self, hp, dataset)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build a Pipeline by Hyperparameters.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m# Arguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    An instance of Pipeline.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m x, y \u001b[38;5;241m=\u001b[39m dataset\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Pipeline(\n\u001b[0;32m---> 64\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_preprocessors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     65\u001b[0m     outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_preprocessors(hp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs, y),\n\u001b[1;32m     66\u001b[0m )\n",
      "File \u001b[0;32m~/tpot_jupyter/miniconda3/envs/ak_env/lib/python3.10/site-packages/autokeras/pipeline.py:46\u001b[0m, in \u001b[0;36mHyperPipeline._build_preprocessors\u001b[0;34m(hp, hpps_lists, dataset)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hyper_preprocessor \u001b[38;5;129;01min\u001b[39;00m hpps_list:\n\u001b[1;32m     45\u001b[0m     preprocessor \u001b[38;5;241m=\u001b[39m hyper_preprocessor\u001b[38;5;241m.\u001b[39mbuild(hp, data)\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     data \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[1;32m     48\u001b[0m     preprocessors\u001b[38;5;241m.\u001b[39mappend(preprocessor)\n",
      "File \u001b[0;32m~/tpot_jupyter/miniconda3/envs/ak_env/lib/python3.10/site-packages/autokeras/preprocessors/common.py:66\u001b[0m, in \u001b[0;36mTextTokenizer.fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     64\u001b[0m unique_words \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m---> 66\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m()\n\u001b[1;32m     67\u001b[0m     unique_words\u001b[38;5;241m.\u001b[39mextend(words)\n\u001b[1;32m     68\u001b[0m word_counts \u001b[38;5;241m=\u001b[39m Counter(unique_words)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 0) Imports\n",
    "# =========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import autokeras as ak\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 1) Read TRAIN / TEST (TSV)\n",
    "# =========================================\n",
    "train_df = pd.read_csv(\n",
    "    \"new_data_train_Yelp_Fake_Review.csv\",\n",
    "    sep=\"\\t\",\n",
    "    engine=\"python\"\n",
    ")\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    \"new_data_test_Yelp_Fake_Review.csv\",\n",
    "    sep=\"\\t\",\n",
    "    engine=\"python\"\n",
    ")\n",
    "\n",
    "train_df = train_df[[\"reviewContent\", \"flagged\"]].dropna().reset_index(drop=True)\n",
    "test_df  = test_df[[\"reviewContent\", \"flagged\"]].dropna().reset_index(drop=True)\n",
    "\n",
    "train_df[\"reviewContent\"] = train_df[\"reviewContent\"].astype(str)\n",
    "test_df[\"reviewContent\"]  = test_df[\"reviewContent\"].astype(str)\n",
    "\n",
    "train_df[\"flagged\"] = train_df[\"flagged\"].astype(int)\n",
    "test_df[\"flagged\"]  = test_df[\"flagged\"].astype(int)\n",
    "\n",
    "print(\"Train labels:\", np.unique(train_df[\"flagged\"], return_counts=True))\n",
    "print(\"Test labels :\", np.unique(test_df[\"flagged\"], return_counts=True))\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 2) Text cleaning\n",
    "# =========================================\n",
    "_url_re = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "_html_re = re.compile(r\"<.*?>\")\n",
    "_space_re = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = s.replace(\"\\t\", \" \").replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    s = _html_re.sub(\" \", s)\n",
    "    s = _url_re.sub(\" URL \", s)\n",
    "    s = _space_re.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "train_df[\"review_clean\"] = train_df[\"reviewContent\"].map(clean_text)\n",
    "test_df[\"review_clean\"]  = test_df[\"reviewContent\"].map(clean_text)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 3) Structured feature engineering\n",
    "# =========================================\n",
    "def add_text_features(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    text = df[col].fillna(\"\").astype(str)\n",
    "\n",
    "    df[\"char_len\"] = text.str.len()\n",
    "    df[\"word_count\"] = text.str.split().map(len)\n",
    "    df[\"exclaim_count\"] = text.str.count(\"!\")\n",
    "    df[\"question_count\"] = text.str.count(r\"\\?\")\n",
    "    df[\"upper_count\"] = text.str.count(r\"[A-Z]\")\n",
    "    df[\"digit_count\"] = text.str.count(r\"\\d\")\n",
    "\n",
    "    df[\"upper_ratio\"] = df[\"upper_count\"] / (df[\"char_len\"] + 1)\n",
    "    df[\"digit_ratio\"] = df[\"digit_count\"] / (df[\"char_len\"] + 1)\n",
    "\n",
    "    df[\"multi_exclaim\"] = text.str.contains(r\"!!+\").astype(int)\n",
    "    df[\"multi_question\"] = text.str.contains(r\"\\?\\?+\").astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = add_text_features(train_df, \"review_clean\")\n",
    "test_df  = add_text_features(test_df,  \"review_clean\")\n",
    "\n",
    "feature_cols = [\n",
    "    \"char_len\", \"word_count\",\n",
    "    \"exclaim_count\", \"question_count\",\n",
    "    \"upper_ratio\", \"digit_ratio\",\n",
    "    \"multi_exclaim\", \"multi_question\"\n",
    "]\n",
    "\n",
    "for c in feature_cols:\n",
    "    train_df[c] = pd.to_numeric(train_df[c], errors=\"coerce\").fillna(0)\n",
    "    test_df[c]  = pd.to_numeric(test_df[c], errors=\"coerce\").fillna(0)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 4) Prepare inputs (CRITICAL SECTION)\n",
    "# =========================================\n",
    "# Text MUST be (N, 1)\n",
    "x_train_text = (\n",
    "    train_df[\"review_clean\"]\n",
    "    .astype(str)\n",
    "    .to_numpy(dtype=str)\n",
    "    .reshape(-1, 1)\n",
    ")\n",
    "\n",
    "x_test_text = (\n",
    "    test_df[\"review_clean\"]\n",
    "    .astype(str)\n",
    "    .to_numpy(dtype=str)\n",
    "    .reshape(-1, 1)\n",
    ")\n",
    "\n",
    "# Structured MUST be NumPy arrays\n",
    "x_train_struct = train_df[feature_cols].astype(np.float32).to_numpy()\n",
    "x_test_struct  = test_df[feature_cols].astype(np.float32).to_numpy()\n",
    "\n",
    "y_train = train_df[\"flagged\"].to_numpy()\n",
    "y_test  = test_df[\"flagged\"].to_numpy()\n",
    "\n",
    "print(\"Text input shape:\", x_train_text.shape)\n",
    "print(\"Struct input shape:\", x_train_struct.shape)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 5) Build MULTIMODAL AutoKeras model\n",
    "# =========================================\n",
    "text_in = ak.TextInput()\n",
    "struct_in = ak.StructuredDataInput()\n",
    "\n",
    "text_feat = ak.TextBlock()(text_in)\n",
    "struct_feat = ak.StructuredDataBlock()(struct_in)\n",
    "\n",
    "merged = ak.Merge()([text_feat, struct_feat])\n",
    "out = ak.ClassificationHead(num_classes=2)(merged)\n",
    "\n",
    "automodel = ak.AutoModel(\n",
    "    inputs=[text_in, struct_in],\n",
    "    outputs=out,\n",
    "    max_trials=10,        # increase to 30–50 later\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 6) Train (LIST INPUT — IMPORTANT)\n",
    "# =========================================\n",
    "automodel.fit(\n",
    "    x=[x_train_text, x_train_struct],\n",
    "    y=y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=5              # increase to 10–20 later\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 7) Predict + Evaluate\n",
    "# =========================================\n",
    "pred = automodel.predict([x_test_text, x_test_struct])\n",
    "pred = np.array(pred)\n",
    "\n",
    "if pred.ndim == 2 and pred.shape[1] == 2:\n",
    "    y_pred = np.argmax(pred, axis=1)\n",
    "else:\n",
    "    y_pred = (pred.reshape(-1) >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1:\", f1_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 8) Save best model (Keras 3 compatible)\n",
    "# =========================================\n",
    "best_model = automodel.export_model()\n",
    "best_model.save(\"autokeras_yelp_multimodal_model.keras\")\n",
    "\n",
    "print(\"\\n✅ Saved: autokeras_yelp_multimodal_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fa60916-f300-4bef-9bc1-f44af7d794e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels: (array([0, 1]), array([4993, 4933]))\n",
      "Test labels : (array([0, 1]), array([1212, 1271]))\n",
      "GPU devices: []\n",
      "Text shape: (9926,) dtype: <U3676\n",
      "Struct shape: (9926, 8) dtype: float32\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expect the data to TextInput to have shape (batch_size, 1), but got input shape [9926, 8].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 120\u001b[0m\n\u001b[1;32m    110\u001b[0m automodel \u001b[38;5;241m=\u001b[39m ak\u001b[38;5;241m.\u001b[39mAutoModel(\n\u001b[1;32m    111\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m[text_in, struct_in],\n\u001b[1;32m    112\u001b[0m     outputs\u001b[38;5;241m=\u001b[39mout,\n\u001b[1;32m    113\u001b[0m     max_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,      \u001b[38;5;66;03m# increase later (30-50)\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    115\u001b[0m )\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# 6) Train (DICT with named inputs)\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[43mautomodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_struct\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m    125\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# 7) Predict + Evaluate\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m    130\u001b[0m pred \u001b[38;5;241m=\u001b[39m automodel\u001b[38;5;241m.\u001b[39mpredict({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: x_test_text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstruct\u001b[39m\u001b[38;5;124m\"\u001b[39m: x_test_struct})\n",
      "File \u001b[0;32m~/tpot_jupyter/miniconda3/envs/ak_env/lib/python3.10/site-packages/autokeras/auto_model.py:293\u001b[0m, in \u001b[0;36mAutoModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, callbacks, validation_split, validation_data, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m     validation_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    290\u001b[0m dataset, validation_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_adapt(\n\u001b[1;32m    291\u001b[0m     x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my, validation_data\u001b[38;5;241m=\u001b[39mvalidation_data\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_analyze_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_hyper_pipeline()\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Split the data with validation_split.\u001b[39;00m\n",
      "File \u001b[0;32m~/tpot_jupyter/miniconda3/envs/ak_env/lib/python3.10/site-packages/autokeras/auto_model.py:381\u001b[0m, in \u001b[0;36mAutoModel._analyze_data\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    378\u001b[0m     analyser\u001b[38;5;241m.\u001b[39mupdate(array)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m analyser \u001b[38;5;129;01min\u001b[39;00m analysers:\n\u001b[0;32m--> 381\u001b[0m     \u001b[43manalyser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hm, analyser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_heads, analysers):\n\u001b[1;32m    384\u001b[0m     hm\u001b[38;5;241m.\u001b[39mconfig_from_analyser(analyser)\n",
      "File \u001b[0;32m~/tpot_jupyter/miniconda3/envs/ak_env/lib/python3.10/site-packages/autokeras/analysers/input_analysers.py:50\u001b[0m, in \u001b[0;36mTextAnalyser.finalize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfinalize\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorrect_shape():\n\u001b[0;32m---> 50\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     51\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpect the data to TextInput to have shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(batch_size, 1), but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot input shape \u001b[39m\u001b[38;5;132;01m{shape}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     54\u001b[0m         )\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpect the data to TextInput to be strings, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{type}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     59\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Expect the data to TextInput to have shape (batch_size, 1), but got input shape [9926, 8]."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import autokeras as ak\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Read TRAIN / TEST (TSV)\n",
    "# -----------------------------\n",
    "train_df = pd.read_csv(\"new_data_train_Yelp_Fake_Review.csv\", sep=\"\\t\", engine=\"python\")\n",
    "test_df  = pd.read_csv(\"new_data_test_Yelp_Fake_Review.csv\",  sep=\"\\t\", engine=\"python\")\n",
    "\n",
    "train_df = train_df[[\"reviewContent\", \"flagged\"]].dropna().reset_index(drop=True)\n",
    "test_df  = test_df[[\"reviewContent\", \"flagged\"]].dropna().reset_index(drop=True)\n",
    "\n",
    "train_df[\"reviewContent\"] = train_df[\"reviewContent\"].astype(str)\n",
    "test_df[\"reviewContent\"]  = test_df[\"reviewContent\"].astype(str)\n",
    "train_df[\"flagged\"] = train_df[\"flagged\"].astype(int)\n",
    "test_df[\"flagged\"]  = test_df[\"flagged\"].astype(int)\n",
    "\n",
    "print(\"Train labels:\", np.unique(train_df[\"flagged\"], return_counts=True))\n",
    "print(\"Test labels :\", np.unique(test_df[\"flagged\"], return_counts=True))\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Text cleaning (light)\n",
    "# -----------------------------\n",
    "_url_re = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "_html_re = re.compile(r\"<.*?>\")\n",
    "_space_re = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = s.replace(\"\\t\", \" \").replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    s = _html_re.sub(\" \", s)\n",
    "    s = _url_re.sub(\" URL \", s)\n",
    "    s = _space_re.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "train_df[\"review_clean\"] = train_df[\"reviewContent\"].map(clean_text)\n",
    "test_df[\"review_clean\"]  = test_df[\"reviewContent\"].map(clean_text)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Structured feature engineering\n",
    "# -----------------------------\n",
    "def add_text_features(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    text = df[col].fillna(\"\").astype(str)\n",
    "\n",
    "    df[\"char_len\"] = text.str.len()\n",
    "    df[\"word_count\"] = text.str.split().map(len)\n",
    "\n",
    "    df[\"exclaim_count\"] = text.str.count(\"!\")\n",
    "    df[\"question_count\"] = text.str.count(r\"\\?\")\n",
    "    df[\"upper_count\"] = text.str.count(r\"[A-Z]\")\n",
    "    df[\"digit_count\"] = text.str.count(r\"\\d\")\n",
    "\n",
    "    df[\"upper_ratio\"] = df[\"upper_count\"] / (df[\"char_len\"] + 1)\n",
    "    df[\"digit_ratio\"] = df[\"digit_count\"] / (df[\"char_len\"] + 1)\n",
    "\n",
    "    df[\"multi_exclaim\"] = text.str.contains(r\"!!+\").astype(int)\n",
    "    df[\"multi_question\"] = text.str.contains(r\"\\?\\?+\").astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = add_text_features(train_df, \"review_clean\")\n",
    "test_df  = add_text_features(test_df,  \"review_clean\")\n",
    "\n",
    "feature_cols = [\n",
    "    \"char_len\", \"word_count\", \"exclaim_count\", \"question_count\",\n",
    "    \"upper_ratio\", \"digit_ratio\", \"multi_exclaim\", \"multi_question\"\n",
    "]\n",
    "\n",
    "for c in feature_cols:\n",
    "    train_df[c] = pd.to_numeric(train_df[c], errors=\"coerce\").fillna(0)\n",
    "    test_df[c]  = pd.to_numeric(test_df[c], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Prepare inputs for AutoKeras (FIXED)\n",
    "#   - Text MUST be 1D array of strings (N,)\n",
    "#   - Structured MUST be numpy float array (N, 8)\n",
    "# -----------------------------\n",
    "x_train_text = train_df[\"review_clean\"].astype(str).to_numpy(dtype=str)     # (N,)\n",
    "x_test_text  = test_df[\"review_clean\"].astype(str).to_numpy(dtype=str)      # (M,)\n",
    "\n",
    "x_train_struct = train_df[feature_cols].astype(np.float32).to_numpy()       # (N, 8)\n",
    "x_test_struct  = test_df[feature_cols].astype(np.float32).to_numpy()        # (M, 8)\n",
    "\n",
    "y_train = train_df[\"flagged\"].to_numpy()\n",
    "y_test  = test_df[\"flagged\"].to_numpy()\n",
    "\n",
    "print(\"Text shape:\", x_train_text.shape, \"dtype:\", x_train_text.dtype)\n",
    "print(\"Struct shape:\", x_train_struct.shape, \"dtype:\", x_train_struct.dtype)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Build Multi-Input AutoKeras model (FIXED)\n",
    "#   - Use scalar text input: shape=()\n",
    "#   - Use named inputs and feed a dict\n",
    "# -----------------------------\n",
    "text_in = ak.TextInput(name=\"text\", shape=())\n",
    "struct_in = ak.StructuredDataInput(name=\"struct\")\n",
    "\n",
    "text_feat = ak.TextBlock()(text_in)\n",
    "struct_feat = ak.StructuredDataBlock()(struct_in)\n",
    "\n",
    "merged = ak.Merge()([text_feat, struct_feat])\n",
    "out = ak.ClassificationHead(num_classes=2)(merged)\n",
    "\n",
    "automodel = ak.AutoModel(\n",
    "    inputs=[text_in, struct_in],\n",
    "    outputs=out,\n",
    "    max_trials=10,      # increase later (30-50)\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Train (DICT with named inputs)\n",
    "# -----------------------------\n",
    "automodel.fit(\n",
    "    x={\"text\": x_train_text, \"struct\": x_train_struct},\n",
    "    y=y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Predict + Evaluate\n",
    "# -----------------------------\n",
    "pred = automodel.predict({\"text\": x_test_text, \"struct\": x_test_struct})\n",
    "pred = np.array(pred)\n",
    "\n",
    "if pred.ndim == 2 and pred.shape[1] == 2:\n",
    "    y_pred = np.argmax(pred, axis=1)\n",
    "else:\n",
    "    y_pred = (pred.reshape(-1) >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1:\", f1_score(y_test, y_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Save best model (Keras 3)\n",
    "# -----------------------------\n",
    "best_model = automodel.export_model()\n",
    "best_model.save(\"autokeras_yelp_multimodal_model.keras\")\n",
    "print(\"\\n✅ Saved: autokeras_yelp_multimodal_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10286ded-9ebc-4368-ad43-7cc5af414aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ak_env)",
   "language": "python",
   "name": "ak_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
